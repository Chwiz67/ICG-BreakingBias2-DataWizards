{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    "SEQUENCE_LENGTH = 16\n",
    "DATASET_DIR = '' # Change this to the path of the dataset\n",
    "CLASSES_LIST = ['Brawl', 'Peace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    frames_list = []\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        success, frame = video_reader.read()\n",
    "        if not success:\n",
    "            break\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        normalized_frame = resized_frame / 255\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "    video_reader.release()\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(DATASET_DIR):\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        for file_name in files_list:\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths = create_dataset(DATASET_DIR=DATASET_DIR)\n",
    "one_hot_encoded_labels = to_categorical(labels)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def build_optimized_slowfast_improved(input_shape, num_classes, alpha=4):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    slow = layers.Lambda(lambda x: x[:, ::alpha, :, :, :])(inputs)\n",
    "    slow = layers.Conv3D(32, (1, 7, 7), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(slow)\n",
    "    slow = layers.BatchNormalization()(slow)\n",
    "    slow = layers.MaxPooling3D((1, 2, 2), padding='same')(slow)\n",
    "    \n",
    "    fast = inputs\n",
    "    fast = layers.Conv3D(16, (1, 5, 5), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(fast)\n",
    "    fast = layers.BatchNormalization()(fast)\n",
    "    fast = layers.MaxPooling3D((1, 2, 2), padding='same')(fast)\n",
    "    fast = layers.MaxPooling3D(pool_size=(alpha, 1, 1), padding='same')(fast)\n",
    "    \n",
    "    slow = layers.Conv3D(16, (1, 1, 1), activation='relu', padding='same')(slow)\n",
    "    fast = layers.Conv3D(16, (1, 1, 1), activation='relu', padding='same')(fast)\n",
    "    \n",
    "    fusion = layers.Concatenate(axis=-1)([slow, fast])\n",
    "    \n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(fusion)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "    x = layers.SpatialDropout3D(0.2)(x)\n",
    "    \n",
    "    shortcut = x\n",
    "    x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling3D((2, 2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=AdamW(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "frame_count = 16\n",
    "height, width, channels = 64, 64, 3\n",
    "num_classes = 2\n",
    "\n",
    "model = build_optimized_slowfast_improved((frame_count, height, width, channels), num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(features_train, labels_train, epochs=100, batch_size=8, verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(features_test)\n",
    "pred = np.argmax(pred, axis = 1)\n",
    "labels_test = np.argmax(labels_test, axis = 1)\n",
    "print(classification_report(labels_test, pred, target_names = CLASSES_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_DATASET_DIR = '' # Change this to the path of the predict dataset\n",
    "predict_features, predict_labels, predict_video_files_paths = create_dataset(DATASET_DIR=PREDICT_DATASET_DIR)\n",
    "predict_labels = to_categorical(predict_labels)\n",
    "\n",
    "pred = model.predict(predict_features)\n",
    "predicted_classes = np.argmax(pred, axis = 1)\n",
    "labels\n",
    "print(classification_report(labels, predicted_classes, target_names = CLASSES_LIST))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
