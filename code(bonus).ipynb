{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"yolov8m.pt\")\n",
    "MoBiLSTM_model = tf.keras.models.load_model('') # Path of the file 'model(bonus).keras'\n",
    "tracker = DeepSort(max_age=100, nn_budget=100, embedder=\"mobilenet\", n_init=3)\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
    "CLASS_LABELS = ['Brawl', 'Peace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_text_size(img_width, base_scale=0.9):  # Increased base_scale for better clarity\n",
    "    \"\"\" Adjust text size dynamically based on image width \"\"\"\n",
    "    scale = max(base_scale * (img_width / 640), 0.7)  # Minimum scale increased\n",
    "    thickness = max(int(scale * 3), 2)  # Increased thickness for visibility\n",
    "    return scale, thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people_and_classify(video_path, output_video_path=\"fight_detection.mp4\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), frame_rate, (width, height))\n",
    "\n",
    "    max_brawl_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = yolo_model(frame)\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        scores = results[0].boxes.conf.cpu().numpy()\n",
    "        class_ids = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "        detections = [\n",
    "            [[box[0], box[1], box[2], box[3]], score, int(cls)]\n",
    "            for box, score, cls in zip(boxes, scores, class_ids) if int(cls) == 0\n",
    "        ]\n",
    "\n",
    "        track_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        current_brawl_count = 0\n",
    "\n",
    "        for track in track_objects:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, ltrb)\n",
    "\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            if person_crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            person_crop = cv2.resize(person_crop, (IMAGE_HEIGHT, IMAGE_WIDTH)) / 255.0\n",
    "            person_crop = np.expand_dims(person_crop, axis=0)\n",
    "\n",
    "            pred = MoBiLSTM_model.predict(np.array([person_crop]))\n",
    "            label_index = np.argmax(pred)\n",
    "            label = CLASS_LABELS[label_index]\n",
    "\n",
    "            if label == \"Brawl\":\n",
    "                current_brawl_count += 1\n",
    "                color = (0, 0, 255)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
    "\n",
    "                font_scale, thickness = auto_text_size(width)\n",
    "                cv2.putText(frame, f\"ID {track_id}: {label}\", (x1, y1 - 15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        max_brawl_count = max(max_brawl_count, current_brawl_count)\n",
    "\n",
    "        font_scale, thickness = auto_text_size(width)\n",
    "\n",
    "        cv2.putText(frame, f\"Max Brawl Count: {max_brawl_count}\", (0, 0),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, int(font_scale/2), (255, 0, 0), int(thickness/5), cv2.LINE_AA)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Number of People involved in brawl: {max_brawl_count}\")\n",
    "    print(f\"Fight detection video saved as {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '' # Path of any video file\n",
    "detect_people_and_classify(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
